{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathiusjohnson/projects/llm-prototyping/venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "INFO:__main__:Loading embedding model...\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:__main__:Loading text files...\n",
      "INFO:__main__:Loaded 4280 paragraphs from Autobiography-of-a-Yogi-by-Paramhansa-Yogananda.txt\n",
      "INFO:__main__:Loaded 9551 paragraphs from vedic_hymns_pt_2.txt\n",
      "ERROR:__main__:Error loading datasets/spiritual_texts/Hildegard Writings.txt: 'utf-8' codec can't decode byte 0x92 in position 347: invalid start byte\n",
      "INFO:__main__:Loaded 862 paragraphs from History of Zoroastrianism - M.N. Dhalla.txt\n",
      "ERROR:__main__:Error loading datasets/spiritual_texts/kitab i ilqan book of certitude.txt: 'utf-8' codec can't decode byte 0x97 in position 646: invalid start byte\n",
      "INFO:__main__:Loaded 2774 paragraphs from Occult_Theocracy.txt\n",
      "INFO:__main__:Loaded 11290 paragraphs from vedic_hymns_pt_1.txt\n",
      "INFO:__main__:Loaded 5223 paragraphs from 7 Tablets of Creation.txt\n",
      "INFO:__main__:Loaded 82 paragraphs from Advaita_Vedanta.txt\n",
      "ERROR:__main__:Error loading datasets/spiritual_texts/The Kitab I Aqdas.txt: 'utf-8' codec can't decode byte 0x92 in position 80: invalid start byte\n",
      "INFO:__main__:Loaded 489 paragraphs from maha-yana_manual.txt\n",
      "ERROR:__main__:Error loading datasets/spiritual_texts/book of illumination kaballah text.txt: 'utf-8' codec can't decode byte 0xad in position 25487: invalid start byte\n",
      "ERROR:__main__:Error loading datasets/spiritual_texts/buddhidm in its connexion with brahmanism.txt: 'utf-8' codec can't decode byte 0xd6 in position 2008: invalid continuation byte\n",
      "INFO:__main__:Loaded 390 paragraphs from Kybalion.txt\n",
      "INFO:__main__:Loaded 74 paragraphs from Kaivalya_Upanishad.txt\n",
      "INFO:__main__:Loaded 608 paragraphs from Hidden Nature - The Startling Insights of Viktor Schauberger - by Alick Bartholomew.txt\n",
      "ERROR:__main__:Error loading datasets/spiritual_texts/vendanta sutras patanjali.txt: 'utf-8' codec can't decode byte 0xc2 in position 1463: invalid continuation byte\n",
      "INFO:__main__:Loaded 87 paragraphs from nature-gods.txt\n",
      "INFO:__main__:Loaded 592 paragraphs from 7 Tablets of Creation vol 2.txt\n",
      "INFO:__main__:Loaded 236 paragraphs from Knowledge of the Higher Worlds - by Rudolf Steiner.txt\n",
      "INFO:__main__:Loaded 19966 paragraphs from The Chaldean Account of Genesis - by George Smith.txt\n",
      "INFO:__main__:Loaded 57 paragraphs from Secret Teachings of the Society of Jesus.txt\n",
      "INFO:__main__:Loaded 2093 paragraphs from lob.txt\n",
      "INFO:__main__:Loaded 1161 paragraphs from Machine_Super_Intelligence.txt\n",
      "INFO:__main__:Loaded 388 paragraphs from Kularnava-Tantra.txt\n",
      "INFO:__main__:Loaded 123 paragraphs from Popol Vuh.txt\n",
      "INFO:__main__:Loaded 462 paragraphs from Collected Fruits of Occult Teaching by A.P.Sinnett (1920).txt\n",
      "INFO:__main__:Loaded 1183 paragraphs from way_of_virtue.txt\n",
      "INFO:__main__:Loaded 6854 paragraphs from Aryan Sun Myths - Charles Morris (1889).txt\n",
      "INFO:__main__:Loaded 3260 paragraphs from dhammapada.txt\n",
      "INFO:__main__:Loaded 594 paragraphs from yoga sutras patanjali.txt\n",
      "ERROR:__main__:Error loading datasets/spiritual_texts/augustine city of god and christian doctrine.txt: 'utf-8' codec can't decode byte 0xe6 in position 2263: invalid continuation byte\n",
      "INFO:__main__:Loaded 676 paragraphs from Bhagavad Gita.txt\n",
      "ERROR:__main__:Error loading datasets/spiritual_texts/epistle to the son of the wolf.txt: 'utf-8' codec can't decode byte 0x92 in position 843: invalid start byte\n",
      "INFO:__main__:Loaded 38819 paragraphs from Bible_King_James_Version.txt\n",
      "INFO:__main__:Loading CSV files...\n",
      "ERROR:__main__:Error loading CSV/Excel files: 'quote'\n",
      "INFO:__main__:Creating embeddings for 112174 texts...\n",
      "Batches:   2%|‚ñè         | 62/3506 [01:49<1:41:46,  1.77s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Create embeddings\u001b[39;00m\n\u001b[1;32m     79\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating embeddings for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m texts...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Create knowledge base\u001b[39;00m\n\u001b[1;32m     83\u001b[0m knowledge_base \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtexts\u001b[39m\u001b[38;5;124m\"\u001b[39m: all_texts,\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msources\u001b[39m\u001b[38;5;124m\"\u001b[39m: all_sources,\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: embeddings,\n\u001b[1;32m     87\u001b[0m }\n",
      "File \u001b[0;32m~/projects/llm-prototyping/venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:650\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[1;32m    649\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n\u001b[0;32m--> 650\u001b[0m                 embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m         all_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    654\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(length_sorted_idx)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# embeddings.ipynb\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add your Django project to Python path\n",
    "project_path = Path.cwd()  # Go up one directory level\n",
    "sys.path.append(str(project_path))\n",
    "\n",
    "# Setup Django environment\n",
    "import os\n",
    "os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')  # adjust if your settings are different\n",
    "\n",
    "# Now you can import your Django models/settings if needed\n",
    "\n",
    "# Import required libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load embedding model\n",
    "logger.info(\"Loading embedding model...\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load texts\n",
    "base_path = Path(\"datasets\")\n",
    "all_texts = []\n",
    "all_sources = []\n",
    "\n",
    "# Load text files from spiritual_texts directory\n",
    "spiritual_texts_path = base_path / \"spiritual_texts\"\n",
    "if spiritual_texts_path.exists():\n",
    "    logger.info(\"Loading text files...\")\n",
    "    for txt_file in spiritual_texts_path.glob(\"*.txt\"):\n",
    "        try:\n",
    "            with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                paragraphs = [p.strip() for p in content.split(\"\\n\\n\") if p.strip()]\n",
    "                all_texts.extend(paragraphs)\n",
    "                all_sources.extend([txt_file.stem] * len(paragraphs))\n",
    "            logger.info(f\"Loaded {len(paragraphs)} paragraphs from {txt_file.name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {txt_file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Load CSVs\n",
    "try:\n",
    "    logger.info(\"Loading CSV files...\")\n",
    "    # Buddha quotes\n",
    "    buddha_quotes = pd.read_csv(base_path / \"buddha_quotes.csv\")\n",
    "    all_texts.extend(buddha_quotes[\"quote\"].tolist())\n",
    "    all_sources.extend([\"Buddha\"] * len(buddha_quotes))\n",
    "\n",
    "    # Asana benefits\n",
    "    asana_benefits = pd.read_csv(base_path / \"asana_benefits.csv\")\n",
    "    all_texts.extend(asana_benefits[\"description\"].tolist())\n",
    "    all_sources.extend([\"Yoga\"] * len(asana_benefits))\n",
    "\n",
    "    # Meditation\n",
    "    meditation = pd.read_csv(base_path / \"meditation.csv\")\n",
    "    all_texts.extend(meditation[\"text\"].tolist())\n",
    "    all_sources.extend([\"Meditation\"] * len(meditation))\n",
    "\n",
    "    # Rumi poetry\n",
    "    rumi_poetry = pd.read_excel(base_path / \"rumi_poetry.xlsx\")\n",
    "    all_texts.extend(rumi_poetry[\"poem\"].tolist())\n",
    "    all_sources.extend([\"Rumi\"] * len(rumi_poetry))\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading CSV/Excel files: {str(e)}\")\n",
    "\n",
    "# Create embeddings\n",
    "logger.info(f\"Creating embeddings for {len(all_texts)} texts...\")\n",
    "embeddings = model.encode(all_texts, show_progress_bar=True)\n",
    "\n",
    "# Create knowledge base\n",
    "knowledge_base = {\n",
    "    \"texts\": all_texts,\n",
    "    \"sources\": all_sources,\n",
    "    \"embeddings\": embeddings,\n",
    "}\n",
    "\n",
    "# Save embeddings\n",
    "cache_path = base_path / \"cached_embeddings.pkl\"\n",
    "logger.info(f\"Saving embeddings to {cache_path}\")\n",
    "with open(cache_path, \"wb\") as f:\n",
    "    pickle.dump(knowledge_base, f)\n",
    "\n",
    "logger.info(\"Done! Embeddings saved successfully.\")\n",
    "\n",
    "# Optional: Test the embeddings\n",
    "test_query = \"What is the meaning of life?\"\n",
    "query_embedding = model.encode(test_query)\n",
    "similarities = np.dot(embeddings, query_embedding)\n",
    "top_indices = np.argsort(similarities)[-3:][::-1]\n",
    "\n",
    "print(\"\\nTesting embeddings with query:\", test_query)\n",
    "for idx in top_indices:\n",
    "    print(f\"\\nFrom {all_sources[idx]}:\")\n",
    "    print(all_texts[idx])\n",
    "    print(f\"Similarity score: {similarities[idx]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
